---
phase: 03-personal-tax-simple
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - src/documents/classifier.py
  - src/documents/confidence.py
autonomous: true

must_haves:
  truths:
    - "Document images can be classified into W-2, 1099-INT, 1099-DIV, 1099-NEC, or UNKNOWN"
    - "Classification returns confidence score between 0.0 and 1.0"
    - "Extraction confidence is calculated from LLM confidence + validation + critical fields"
    - "Confidence levels are HIGH (>=0.85), MEDIUM (>=0.60), LOW (<0.60)"
  artifacts:
    - path: "src/documents/classifier.py"
      provides: "Document type classification using Claude Vision"
      exports: ["classify_document", "ClassificationResult"]
    - path: "src/documents/confidence.py"
      provides: "Confidence scoring logic for extractions"
      exports: ["calculate_confidence", "ConfidenceResult", "ConfidenceLevel"]
  key_links:
    - from: "src/documents/classifier.py"
      to: "src/documents/models.py"
      via: "DocumentType enum"
      pattern: "DocumentType"
    - from: "src/documents/classifier.py"
      to: "instructor"
      via: "Claude API wrapper"
      pattern: "instructor.from_anthropic"
---

<objective>
Create document classifier and confidence scoring for extraction reliability.

Purpose: Classification routes documents to the correct extraction model, and confidence scoring evaluates whether extractions need human review. Required for DOC-06 (classifier) and DOC-07 (confidence scoring).

Output:
- src/documents/classifier.py with ClassificationResult and classify_document function
- src/documents/confidence.py with ConfidenceResult and calculate_confidence function
</objective>

<execution_context>
@/Users/tungmbp1423/.claude/get-shit-done/workflows/execute-plan.md
@/Users/tungmbp1423/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-personal-tax-simple/03-RESEARCH.md
@src/documents/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create document classifier</name>
  <files>src/documents/classifier.py</files>
  <action>
Create classifier.py with Claude Vision-based document classification.

**ClassificationResult model:**
```python
class ClassificationResult(BaseModel):
    """Document classification result."""
    document_type: DocumentType
    confidence: float = Field(ge=0.0, le=1.0)
    reasoning: str
```

**classify_document function:**
```python
async def classify_document(
    image_bytes: bytes,
    media_type: str = "image/jpeg",
    client: AsyncAnthropic | None = None,
) -> ClassificationResult:
    """Classify tax document type from image using Claude Vision.

    Args:
        image_bytes: Document image as bytes
        media_type: MIME type (image/jpeg, image/png, application/pdf)
        client: Optional Anthropic client (for dependency injection in tests)

    Returns:
        ClassificationResult with document_type, confidence, reasoning
    """
```

**Implementation:**
- Use instructor.from_anthropic(AsyncAnthropic()) for structured output
- Model: claude-sonnet-4-5-20250514 (faster for classification)
- Max tokens: 512 (classification doesn't need much)
- Prompt should describe distinguishing features:
  - W-2: "Wage and Tax Statement", boxes labeled 1-20
  - 1099-INT: "Interest Income", interest boxes
  - 1099-DIV: "Dividends and Distributions", boxes 1a, 1b, 2a
  - 1099-NEC: "Nonemployee Compensation", box 1
- Return UNKNOWN with low confidence if cannot determine
- Handle base64 encoding of image

**Mock mode:**
Add environment variable check for MOCK_LLM=true to return mock classification for testing without API key.

Update src/documents/__init__.py to export classifier types.
  </action>
  <verify>
```bash
# Test with mock mode (no API call)
MOCK_LLM=true uv run python -c "
import asyncio
from src.documents.classifier import classify_document, ClassificationResult
from src.documents.models import DocumentType

async def test():
    # Mock mode should return a result without API call
    result = await classify_document(b'fake image bytes', 'image/jpeg')
    print(f'Type: {result.document_type}')
    print(f'Confidence: {result.confidence}')
    print(f'Reasoning: {result.reasoning}')
    assert isinstance(result, ClassificationResult)
    print('Classifier mock mode OK')

asyncio.run(test())
"
```
  </verify>
  <done>classify_document returns ClassificationResult with document_type, confidence, reasoning; mock mode works</done>
</task>

<task type="auto">
  <name>Task 2: Create confidence scoring module</name>
  <files>src/documents/confidence.py</files>
  <action>
Create confidence.py with extraction confidence calculation.

**ConfidenceLevel enum:**
```python
class ConfidenceLevel(str, Enum):
    HIGH = "HIGH"
    MEDIUM = "MEDIUM"
    LOW = "LOW"
```

**ConfidenceResult dataclass:**
```python
@dataclass
class ConfidenceResult:
    level: ConfidenceLevel
    score: float  # 0.0 to 1.0
    factors: dict[str, float]  # Individual factor scores
    notes: list[str]  # Explanation notes
```

**calculate_confidence function:**
```python
def calculate_confidence(
    llm_reported_confidence: str,
    field_validations: dict[str, bool],
    critical_fields_present: dict[str, bool],
    critical_field_names: list[str] | None = None,
) -> ConfidenceResult:
    """Calculate overall extraction confidence.

    Weights:
    - LLM self-reported confidence: 0.3
    - Field format validation pass rate: 0.4
    - Critical field presence: 0.3

    Args:
        llm_reported_confidence: "HIGH", "MEDIUM", or "LOW" from extraction
        field_validations: Dict of field_name -> passed_validation
        critical_fields_present: Dict of critical_field -> is_present
        critical_field_names: List of field names considered critical

    Returns:
        ConfidenceResult with level, score, factors breakdown

    Thresholds:
    - HIGH: score >= 0.85 AND all critical fields present
    - MEDIUM: score >= 0.60
    - LOW: score < 0.60
    """
```

**Default critical fields by document type:**
```python
CRITICAL_FIELDS = {
    DocumentType.W2: ["employee_ssn", "employer_ein", "wages_tips_compensation", "federal_tax_withheld"],
    DocumentType.FORM_1099_INT: ["payer_tin", "recipient_tin", "interest_income"],
    DocumentType.FORM_1099_DIV: ["payer_tin", "recipient_tin", "total_ordinary_dividends"],
    DocumentType.FORM_1099_NEC: ["payer_tin", "recipient_tin", "nonemployee_compensation"],
}
```

**get_critical_fields function:**
```python
def get_critical_fields(document_type: DocumentType) -> list[str]:
    """Get critical field names for document type."""
```

Update src/documents/__init__.py to export confidence types.
  </action>
  <verify>
```bash
uv run python -c "
from src.documents.confidence import calculate_confidence, ConfidenceLevel, ConfidenceResult

# Test HIGH confidence scenario
result = calculate_confidence(
    llm_reported_confidence='HIGH',
    field_validations={'ssn': True, 'ein': True, 'wages': True},
    critical_fields_present={'employee_ssn': True, 'employer_ein': True, 'wages_tips_compensation': True, 'federal_tax_withheld': True},
)
print(f'Score: {result.score:.2f}')
print(f'Level: {result.level}')
print(f'Factors: {result.factors}')
assert result.level == ConfidenceLevel.HIGH, f'Expected HIGH, got {result.level}'

# Test LOW confidence scenario
result_low = calculate_confidence(
    llm_reported_confidence='LOW',
    field_validations={'ssn': False, 'ein': False},
    critical_fields_present={'employee_ssn': False},
)
assert result_low.level == ConfidenceLevel.LOW, f'Expected LOW, got {result_low.level}'

print('Confidence scoring OK')
"
```
  </verify>
  <done>calculate_confidence returns ConfidenceResult with correct level based on weighted factors</done>
</task>

<task type="auto">
  <name>Task 3: Create classifier and confidence tests</name>
  <files>tests/documents/test_classifier.py, tests/documents/test_confidence.py</files>
  <action>
Create tests for classifier and confidence modules.

**test_classifier.py tests:**
- ClassificationResult model validates correctly
- classify_document returns ClassificationResult (mock mode)
- Document type enum values accessible
- Confidence bounded between 0.0 and 1.0
- Mock mode returns deterministic results

**test_confidence.py tests:**
- HIGH confidence: high LLM + all validations pass + all critical fields
- MEDIUM confidence: medium LLM + some validations fail
- LOW confidence: low LLM or critical fields missing
- Score calculation correct for each factor weight (0.3, 0.4, 0.3)
- Critical fields for W2 include SSN, EIN, wages, fed_withheld
- Critical fields for 1099-INT include payer_tin, interest_income
- Critical fields for 1099-DIV include payer_tin, dividends
- Critical fields for 1099-NEC include payer_tin, compensation
- Missing critical field forces non-HIGH even with high score
- Factors dict contains all three factor scores
- Notes populated for LOW confidence cases

Use pytest parametrize for boundary conditions.
  </action>
  <verify>
```bash
uv run pytest tests/documents/test_classifier.py tests/documents/test_confidence.py -v
```
All tests pass.
  </verify>
  <done>20+ tests covering classifier and confidence modules pass</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. Classifier works in mock mode:
```bash
MOCK_LLM=true uv run python -c "
import asyncio
from src.documents import classify_document
asyncio.run(classify_document(b'test', 'image/jpeg'))
print('Classifier OK')
"
```

2. Confidence scoring works:
```bash
uv run python -c "from src.documents import calculate_confidence, ConfidenceLevel; print('Confidence OK')"
```

3. All tests pass:
```bash
uv run pytest tests/documents/test_classifier.py tests/documents/test_confidence.py -v
```
</verification>

<success_criteria>
- classify_document classifies images into DocumentType enum values
- Mock mode enables testing without Anthropic API key
- calculate_confidence returns weighted score with correct thresholds
- HIGH requires score >= 0.85 AND all critical fields present
- 20+ unit tests passing
</success_criteria>

<output>
After completion, create `.planning/phases/03-personal-tax-simple/03-03-SUMMARY.md`
</output>
